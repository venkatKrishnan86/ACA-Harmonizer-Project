{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-19T20:36:01.263591Z",
     "start_time": "2023-11-19T20:35:59.094962Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# !!! Code in this cell is for chunking the dataset into little bits - you'll only need to run this once\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_location = \"../../Datasets/musdb18-wav/train/\"\n",
    "out_location = \"../../Datasets/musdb18-wav/\"\n",
    "folders = os.listdir(data_location)\n",
    "\n",
    "for folder in folders:\n",
    "    other_y, _ = librosa.load(data_location + '/' + folder + '/other.wav', sr=22050)\n",
    "    vocals_y, _ = librosa.load(data_location + '/' + folder + '/vocals.wav', sr=22050)\n",
    "    chunk_length = 22050 * 2\n",
    "    nchunks = int(other_y.shape[0] / chunk_length)\n",
    "\n",
    "    # Path(data_location + folder + '/chunks').mkdir(exist_ok=True)\n",
    "    for chunk in range(nchunks):\n",
    "        other_chunk = other_y[chunk * chunk_length:(chunk * chunk_length) + chunk_length]\n",
    "        vocal_chunk = vocals_y[chunk * chunk_length:(chunk * chunk_length) + chunk_length]\n",
    "        sf.write(out_location + 'chunks_other/chunked_other_' + folder + '_' + str(chunk) + '.wav', other_chunk, 22050)\n",
    "        sf.write(out_location + 'chunks_vocal/chunked_vocal_' + folder + '_' + str(chunk) + '.wav', vocal_chunk, 22050)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T19:10:53.062313Z",
     "start_time": "2023-11-01T19:10:09.625541Z"
    }
   },
   "id": "ba0e9c51f9cdef1a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def chroma_predict(model, audio, chroma_req = True, chord_templates:dict = json.load(open('./chord_templates.json')), sr = 44100, hop = 256):\n",
    "    if chroma_req:\n",
    "        chroma = torch.Tensor(librosa.feature.chroma_cens(y=audio, sr = sr, hop_length=hop)).T.unsqueeze(0)\n",
    "    else:\n",
    "        chroma = audio\n",
    "    with torch.no_grad():\n",
    "        outputs = nn.functional.softmax(model(chroma), 1)[0]\n",
    "    min_val = 120\n",
    "    min_key = ''\n",
    "    for key, val in chord_templates.items():\n",
    "        out = torch.norm(torch.Tensor(val) - outputs)\n",
    "        if min_val >= out:\n",
    "            min_val = out\n",
    "            min_key = key\n",
    "    return min_key\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T20:43:39.879633Z",
     "start_time": "2023-11-19T20:43:39.873088Z"
    }
   },
   "id": "217e82dccf572040"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size = 12, hidden_size = 64, num_layers = 1, num_classes = 12, bidirectional = True) -> None:\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True, bidirectional=bidirectional)\n",
    "        if(bidirectional):\n",
    "            self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(self.bidirectional):\n",
    "            h0 = torch.zeros(2*self.num_layers, x.size(0), self.hidden_size)\n",
    "        else:\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:,-1,:] # Since we only want the output of the last cell\n",
    "        out = self.fc(out)\n",
    "        return(out)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T20:43:41.195866Z",
     "start_time": "2023-11-19T20:43:41.189122Z"
    }
   },
   "id": "6942a376fbccce4a"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "class Musdb18Dataset(Dataset):\n",
    "    def __init__(self, data_location):\n",
    "        super().__init__()\n",
    "        self.path = data_location\n",
    "        self.vocal_files = os.listdir(data_location + '/chunks_vocal')\n",
    "        self.other_files = os.listdir(data_location + '/chunks_other')\n",
    "        self.chroma_gru = GRU()\n",
    "        self.chroma_gru.load_state_dict(torch.load('models/chord_detector.pth'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vocal_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        other_y, sr = librosa.load(self.path + '/chunks_other/' + self.other_files[index], sr=22050)\n",
    "        vocals_y, sr = librosa.load(self.path + '/chunks_vocal/' + self.vocal_files[index], sr=22050)\n",
    "        \n",
    "        other_chroma = librosa.feature.chroma_cens(y=other_y, sr = sr, hop_length=256)\n",
    "        vocals_chroma = librosa.feature.chroma_cens(y=vocals_y, sr = sr, hop_length=256)\n",
    "        \n",
    "        ground_truth = chroma_predict(self.chroma_gru, other_y)\n",
    "        return torch.tensor(vocals_chroma), ground_truth"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T02:31:24.754774Z",
     "start_time": "2023-11-20T02:31:24.752557Z"
    }
   },
   "id": "b7ee83a91370f306"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.f_dim = 19\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(12, 24, 5, 2)\n",
    "        self.conv2 = nn.Conv1d(24, 48, 5, 2)\n",
    "        self.conv3 = nn.Conv1d(48, 12, 5, 2)\n",
    "        self.FC = nn.Linear(self.f_dim * 12, 24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.conv1(x))\n",
    "        h2 = self.relu(self.conv2(h1))\n",
    "        h3 = self.relu(self.conv3(h2))\n",
    "        flat = torch.flatten(h3, 1)\n",
    "        h4 = self.FC(flat)\n",
    "        return self.softmax(h4) \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T02:31:28.156497Z",
     "start_time": "2023-11-20T02:31:28.152351Z"
    }
   },
   "id": "1cc5f87a1e87b7a"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def truth_label_to_int(gt):\n",
    "    template = json.load(open('./chord_templates.json'))\n",
    "    int_gt = torch.zeros(len(gt))\n",
    "    for i in range(len(gt)):\n",
    "        idx = list(template.keys()).index(gt[i])\n",
    "        int_gt[i] = idx\n",
    "    return int_gt.long()\n",
    "    \n",
    "def train():\n",
    "    dataset = Musdb18Dataset('../../Datasets/musdb18-wav')\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=8,\n",
    "                                               shuffle=True)\n",
    "    pred = Predictor()\n",
    "    adam = torch.optim.Adam(pred.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        epoch_loss = 0\n",
    "        count = 0\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            ground_truth = truth_label_to_int(y)\n",
    "            out = pred(X)\n",
    "            loss = loss_fn(out, ground_truth)\n",
    "            loss.backward()\n",
    "            adam.step()\n",
    "            adam.zero_grad()\n",
    "            epoch_loss += loss.detach()\n",
    "            count += 1\n",
    "            print(loss.detach())\n",
    "        print(epoch_loss / count)\n",
    "        \n",
    "    torch.save(pred.state_dict(), 'models/predictor.model')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T02:32:37.199280Z",
     "start_time": "2023-11-20T02:32:37.196851Z"
    }
   },
   "id": "f5448474f43554d9"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pq/fg92p7r56cd82gxyhj7r9s140000gn/T/ipykernel_24541/790191051.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(h4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1787)\n",
      "tensor(3.1783)\n",
      "tensor(3.1783)\n",
      "tensor(3.1782)\n",
      "tensor(3.1793)\n",
      "tensor(3.1782)\n",
      "tensor(3.1780)\n",
      "tensor(3.1789)\n",
      "tensor(3.1784)\n",
      "tensor(3.1782)\n",
      "tensor(3.1790)\n",
      "tensor(3.1774)\n",
      "tensor(3.1791)\n",
      "tensor(3.1780)\n",
      "tensor(3.1787)\n",
      "tensor(3.1785)\n",
      "tensor(3.1775)\n",
      "tensor(3.1788)\n",
      "tensor(3.1779)\n",
      "tensor(3.1780)\n",
      "tensor(3.1784)\n",
      "tensor(3.1776)\n",
      "tensor(3.1784)\n",
      "tensor(3.1796)\n",
      "tensor(3.1781)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[75], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[74], line 21\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     19\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     20\u001B[0m count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (X, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[1;32m     22\u001B[0m     ground_truth \u001B[38;5;241m=\u001B[39m truth_label_to_int(y)\n\u001B[1;32m     23\u001B[0m     out \u001B[38;5;241m=\u001B[39m pred(X)\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[66], line 17\u001B[0m, in \u001B[0;36mMusdb18Dataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     14\u001B[0m other_y, sr \u001B[38;5;241m=\u001B[39m librosa\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/chunks_other/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mother_files[index], sr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m22050\u001B[39m)\n\u001B[1;32m     15\u001B[0m vocals_y, sr \u001B[38;5;241m=\u001B[39m librosa\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/chunks_vocal/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocal_files[index], sr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m22050\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m other_chroma \u001B[38;5;241m=\u001B[39m \u001B[43mlibrosa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchroma_cens\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mother_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m vocals_chroma \u001B[38;5;241m=\u001B[39m librosa\u001B[38;5;241m.\u001B[39mfeature\u001B[38;5;241m.\u001B[39mchroma_cens(y\u001B[38;5;241m=\u001B[39mvocals_y, sr \u001B[38;5;241m=\u001B[39m sr, hop_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m)\n\u001B[1;32m     20\u001B[0m ground_truth \u001B[38;5;241m=\u001B[39m chroma_predict(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchroma_gru, other_y)\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/librosa/feature/spectral.py:1522\u001B[0m, in \u001B[0;36mchroma_cens\u001B[0;34m(y, sr, C, hop_length, fmin, tuning, n_chroma, n_octaves, bins_per_octave, cqt_mode, window, norm, win_len_smooth, smoothing_window)\u001B[0m\n\u001B[1;32m   1514\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\n\u001B[1;32m   1515\u001B[0m     (win_len_smooth \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(win_len_smooth, (\u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39minteger)) \u001B[38;5;129;01mand\u001B[39;00m win_len_smooth \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   1517\u001B[0m ):\n\u001B[1;32m   1518\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ParameterError(\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwin_len_smooth=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwin_len_smooth\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be a positive integer or None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1520\u001B[0m     )\n\u001B[0;32m-> 1522\u001B[0m chroma \u001B[38;5;241m=\u001B[39m \u001B[43mchroma_cqt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1523\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mC\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43msr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfmin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbins_per_octave\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbins_per_octave\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtuning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtuning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_chroma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_chroma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_octaves\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_octaves\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcqt_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcqt_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1534\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# L1-Normalization\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m chroma \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mnormalize(chroma, norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/librosa/feature/spectral.py:1386\u001B[0m, in \u001B[0;36mchroma_cqt\u001B[0;34m(y, sr, C, hop_length, fmin, norm, threshold, tuning, n_chroma, n_octaves, window, bins_per_octave, cqt_mode)\u001B[0m\n\u001B[1;32m   1381\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1382\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ParameterError(\n\u001B[1;32m   1383\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one of C or y must be provided to compute chroma\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1384\u001B[0m         )\n\u001B[1;32m   1385\u001B[0m     C \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mabs(\n\u001B[0;32m-> 1386\u001B[0m         \u001B[43mcqt_func\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcqt_mode\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1387\u001B[0m \u001B[43m            \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1388\u001B[0m \u001B[43m            \u001B[49m\u001B[43msr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1389\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1390\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfmin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1391\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_bins\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_octaves\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbins_per_octave\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1392\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbins_per_octave\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbins_per_octave\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1393\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtuning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtuning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1394\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1395\u001B[0m     )\n\u001B[1;32m   1397\u001B[0m \u001B[38;5;66;03m# Map to chroma\u001B[39;00m\n\u001B[1;32m   1398\u001B[0m cq_to_chr \u001B[38;5;241m=\u001B[39m filters\u001B[38;5;241m.\u001B[39mcq_to_chroma(\n\u001B[1;32m   1399\u001B[0m     C\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m],\n\u001B[1;32m   1400\u001B[0m     bins_per_octave\u001B[38;5;241m=\u001B[39mbins_per_octave,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1403\u001B[0m     window\u001B[38;5;241m=\u001B[39mwindow,\n\u001B[1;32m   1404\u001B[0m )\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/librosa/core/constantq.py:171\u001B[0m, in \u001B[0;36mcqt\u001B[0;34m(y, sr, hop_length, fmin, n_bins, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute the constant-Q transform of an audio signal.\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03mThis implementation is based on the recursive sub-sampling method\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;124;03m       [5.147e-02, 6.959e-02, ..., 1.694e-05, 5.811e-06]])\u001B[39;00m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# CQT is the special case of VQT with gamma=0\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mvqt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m    \u001B[49m\u001B[43msr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfmin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_bins\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_bins\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintervals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mequal\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbins_per_octave\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbins_per_octave\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtuning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtuning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilter_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilter_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43msparsity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparsity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[43mres_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mres_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/librosa/core/constantq.py:1018\u001B[0m, in \u001B[0;36mvqt\u001B[0;34m(y, sr, hop_length, fmin, n_bins, intervals, gamma, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype)\u001B[0m\n\u001B[1;32m   1014\u001B[0m fft_basis[:] \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(sr \u001B[38;5;241m/\u001B[39m my_sr)\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;66;03m# Compute the vqt filter response and append to the stack\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m vqt_resp\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m-> 1018\u001B[0m     \u001B[43m__cqt_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmy_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_fft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmy_hop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfft_basis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1019\u001B[0m )\n\u001B[1;32m   1021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m my_hop \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1022\u001B[0m     my_hop \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/librosa/core/constantq.py:1126\u001B[0m, in \u001B[0;36m__cqt_response\u001B[0;34m(y, n_fft, hop_length, fft_basis, mode, window, phase, dtype)\u001B[0m\n\u001B[1;32m   1124\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute the filter response with a target STFT hop.\"\"\"\u001B[39;00m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;66;03m# Compute the STFT matrix\u001B[39;00m\n\u001B[0;32m-> 1126\u001B[0m D \u001B[38;5;241m=\u001B[39m \u001B[43mstft\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1127\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_fft\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_fft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\n\u001B[1;32m   1128\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m phase:\n\u001B[1;32m   1131\u001B[0m     D \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mabs(D)\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/librosa/core/spectrum.py:378\u001B[0m, in \u001B[0;36mstft\u001B[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m bl_s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, y_frames\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], n_columns):\n\u001B[1;32m    376\u001B[0m     bl_t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(bl_s \u001B[38;5;241m+\u001B[39m n_columns, y_frames\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m--> 378\u001B[0m     stft_matrix[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, bl_s \u001B[38;5;241m+\u001B[39m off_start : bl_t \u001B[38;5;241m+\u001B[39m off_start] \u001B[38;5;241m=\u001B[39m \u001B[43mfft\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrfft\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfft_window\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_frames\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbl_s\u001B[49m\u001B[43m:\u001B[49m\u001B[43mbl_t\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stft_matrix\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mrfft\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/numpy/fft/_pocketfft.py:409\u001B[0m, in \u001B[0;36mrfft\u001B[0;34m(a, n, axis, norm)\u001B[0m\n\u001B[1;32m    407\u001B[0m     n \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mshape[axis]\n\u001B[1;32m    408\u001B[0m inv_norm \u001B[38;5;241m=\u001B[39m _get_forward_norm(n, norm)\n\u001B[0;32m--> 409\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43m_raw_fft\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minv_norm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/Research/venv/lib/python3.9/site-packages/numpy/fft/_pocketfft.py:73\u001B[0m, in \u001B[0;36m_raw_fft\u001B[0;34m(a, n, axis, is_real, is_forward, inv_norm)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     a \u001B[38;5;241m=\u001B[39m swapaxes(a, axis, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 73\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mpfi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_real\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfct\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m     r \u001B[38;5;241m=\u001B[39m swapaxes(r, axis, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T02:33:17.229707Z",
     "start_time": "2023-11-20T02:32:37.537150Z"
    }
   },
   "id": "ea1d2ee003e92b90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2fa691b0b64b1707"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
