{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T20:36:01.263591Z",
     "start_time": "2023-11-19T20:35:59.094962Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf\n",
    "from utils import GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 44100\n",
    "HOP = 256\n",
    "FRAMES = 6\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (gru): GRU(12, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chord_detector = GRU()\n",
    "chord_detector.load_state_dict(torch.load('./models/chord_detector.pth'))\n",
    "chord_detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelChordDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_location = \"../../../../Music Technology/Datasets/musdb18hq/\",\n",
    "            out_location = \"../../../../Music Technology/Datasets/musdb18hq/\",\n",
    "            frames_per_chord = 6,\n",
    "            train = True,\n",
    "            write_data = False\n",
    "        ):\n",
    "        super(MelChordDataset).__init__()\n",
    "        if write_data:\n",
    "            self._write_chords_and_audio(data_location, out_location, train)\n",
    "        self.frames_per_chord = frames_per_chord\n",
    "        vocals_y = []\n",
    "        vocals_chroma = []\n",
    "        chord_templates:dict = json.load(open('./chord_templates.json'))\n",
    "        act_chord_data = []\n",
    "        self.data_location= data_location+\"chunks_vocal/\"\n",
    "        self.out_location= out_location+\"chunks_chord/\"\n",
    "        if not train:\n",
    "            self.data_location[:-1]+=\"_test/\"\n",
    "            self.out_location[:-1]+=\"_test/\"\n",
    "\n",
    "        for i in range(len(os.listdir(self.data_location))): # 100\n",
    "            with open(self.out_location+\"chord_\"+str(i), \"rb\") as fp:\n",
    "                chord_data = pickle.load(fp)\n",
    "            act_chord_data.append(torch.Tensor(np.array([np.array(chord_templates[i]) for i in chord_data])))\n",
    "            vocals_y.append(librosa.load(self.data_location + 'vocal_'+str(i)+'.wav', sr=SR)[0])\n",
    "            vocals_chroma.append(torch.Tensor(librosa.feature.chroma_cens(y=vocals_y[-1], sr = SR, hop_length=HOP)).T)\n",
    "        \n",
    "        # act_chord_data[i]: Shape: (num_chords[i], 12)\n",
    "        # vocals_chroma[i]: Shape: (num_frames[i], 12)\n",
    "        # num_chords[i] = (num_frames[i] // frames_per_chord)\n",
    "\n",
    "        self.data = []\n",
    "        self._create_data(act_chord_data, vocals_chroma)\n",
    "    \n",
    "    def _create_data(self, chord_data, chroma_data):\n",
    "        for (chroma, chords) in zip(chroma_data, chord_data):\n",
    "            for i in range(0, chroma.shape[0]-self.frames_per_chord, self.frames_per_chord):\n",
    "                block_chroma = chroma[i:i+self.frames_per_chord,:]\n",
    "                block_chord = chords[i//self.frames_per_chord]\n",
    "                if(block_chroma.any()):\n",
    "                    self.data.append((block_chroma, block_chord))\n",
    "\n",
    "    def _write_chords_and_audio(\n",
    "            self, \n",
    "            data_location, \n",
    "            out_location, \n",
    "            train = True\n",
    "        ):\n",
    "        if train:\n",
    "            data_location = data_location+\"train/\"\n",
    "        else:\n",
    "            data_location = data_location+\"test/\"\n",
    "        folders = os.listdir(data_location)\n",
    "        count = 0\n",
    "\n",
    "        for folder in folders:\n",
    "            if not os.path.isdir(data_location+folder):\n",
    "                continue\n",
    "            mixture_y, _ = librosa.load(data_location + '/' + folder + '/mixture.wav', sr=SR)\n",
    "            vocals_y, _ = librosa.load(data_location + '/' + folder + '/vocals.wav', sr=SR)\n",
    "            mixture_y = mixture_y/np.max(np.abs(mixture_y))\n",
    "            vocals_y = vocals_y/np.max(np.abs(vocals_y))\n",
    "\n",
    "            mixture_chroma = torch.Tensor(librosa.feature.chroma_cens(y=mixture_y, sr = SR, hop_length=HOP)).T\n",
    "            chunk_length = FRAMES\n",
    "            nchunks = mixture_chroma.shape[0] // chunk_length # no padding\n",
    "\n",
    "            if train:\n",
    "                if not os.path.isdir(out_location+'chunks_chord'):\n",
    "                    os.mkdir(out_location+'chunks_chord')\n",
    "                if not os.path.isdir(out_location+'chunks_vocal'):\n",
    "                    os.mkdir(out_location+'chunks_vocal')\n",
    "            else:\n",
    "                if not os.path.isdir(out_location+'chunks_chord_test/'):\n",
    "                    os.mkdir(out_location+'chunks_chord_test')\n",
    "                if not os.path.isdir(out_location+'chunks_vocal_test/'):\n",
    "                    os.mkdir(out_location+'chunks_vocal_test')\n",
    "\n",
    "            # Get chords from mixture chroma\n",
    "            chord_stack, time = MelChordDataset.prediction(chord_detector, mixture_chroma)\n",
    "            frame_num = np.array([int(i/((HOP/SR)*6)) for i in time])\n",
    "            chord_stack = np.array([frame_num, chord_stack]).T\n",
    "            chords = []\n",
    "            for prev, curr in zip(chord_stack[:-1], chord_stack[1:]):\n",
    "                frame_diff = int(curr[0]) - int(prev[0])\n",
    "                chords.extend([prev[1] for _ in range(frame_diff)])\n",
    "            chords.extend([chord_stack[-1][1] for _ in range(nchunks - len(chords))])\n",
    "\n",
    "            if train:\n",
    "                with open(out_location+\"chunks_chord/chord_\"+str(count), \"wb\") as fp:\n",
    "                    pickle.dump(chords, fp)\n",
    "                sf.write(out_location + 'chunks_vocal/vocal_' + str(count)+'.wav', vocals_y, SR)\n",
    "            else:\n",
    "                with open(out_location+\"chunks_chord_test/chord_\"+str(count), \"wb\") as fp:\n",
    "                    pickle.dump(chords, fp)\n",
    "                sf.write(out_location + 'chunks_vocal_test/vocal_' + str(count)+'.wav', vocals_y, SR)\n",
    "            count+=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(model, audio, chroma_req = True, chord_templates:dict = json.load(open('./chord_templates.json')), sr = SR, hop = HOP):\n",
    "        if chroma_req:\n",
    "            chroma = torch.Tensor(librosa.feature.chroma_cens(y=audio, sr = sr, hop_length=hop)).T.unsqueeze(0)\n",
    "        else:\n",
    "            chroma = audio\n",
    "        with torch.no_grad():\n",
    "            outputs = nn.functional.softmax(model(chroma), 1)[0]\n",
    "        min_val = 120\n",
    "        min_key = ''\n",
    "        for key, val in chord_templates.items():\n",
    "            out = torch.norm(torch.Tensor(val) - outputs)\n",
    "            if min_val >= out:\n",
    "                min_val = out\n",
    "                min_key = key\n",
    "        return min_key\n",
    "    \n",
    "    @staticmethod\n",
    "    def prediction(model, chroma, frame = 6):\n",
    "        stack = []\n",
    "        time = []\n",
    "        model.eval()\n",
    "        pred = MelChordDataset.predict(model, chroma[:frame, :].unsqueeze(0), False)\n",
    "        prev_pred = pred\n",
    "        dur = 1\n",
    "        main_sub = 0\n",
    "        for i in tqdm(range(frame, chroma.shape[0]-frame+1, frame)):\n",
    "            model.eval()\n",
    "            pred = MelChordDataset.predict(model, chroma[i:i+frame, :].unsqueeze(0), False)\n",
    "            if(pred != prev_pred):\n",
    "                if(dur>10):\n",
    "                    if(len(stack)==0):\n",
    "                        stack.append(prev_pred)\n",
    "                    elif(stack[-1]==prev_pred):\n",
    "                        dur = 0\n",
    "                        prev_pred = pred\n",
    "                        continue\n",
    "                    else:\n",
    "                        stack.append(prev_pred)\n",
    "                    if len(time)!=0:\n",
    "                        time.append((i)*HOP/SR - main_sub)\n",
    "                    else:\n",
    "                        main_sub = (i)*HOP/SR\n",
    "                        time.append(0.0)\n",
    "                dur = 0\n",
    "                prev_pred = pred\n",
    "            dur+=1\n",
    "        return stack, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = MelChordDataset(train = True, write_data = False)\n",
    "# test_data = MelChordDataset(train = False, write_data = False)\n",
    "# torch.save(train_data, './data/final/train_data.pt')\n",
    "# torch.save(test_data, './data/final/test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('./data/final/train_data.pt')\n",
    "test_data = torch.load('./data/final/test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576514\n",
      "297423\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6942a376fbccce4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T20:43:41.195866Z",
     "start_time": "2023-11-19T20:43:41.189122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class GRU(nn.Module):\n",
    "#     def __init__(self, input_size = 12, hidden_size = 64, num_layers = 1, num_classes = 12, bidirectional = True) -> None:\n",
    "#         super(GRU, self).__init__()\n",
    "#         self.num_layers = num_layers\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.bidirectional = bidirectional\n",
    "\n",
    "#         self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True, bidirectional=bidirectional)\n",
    "#         if(bidirectional):\n",
    "#             self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "#         else:\n",
    "#             self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if(self.bidirectional):\n",
    "#             h0 = torch.zeros(2*self.num_layers, x.size(0), self.hidden_size)\n",
    "#         else:\n",
    "#             h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "#         out, _ = self.gru(x, h0)\n",
    "#         out = out[:,-1,:] # Since we only want the output of the last cell\n",
    "#         out = self.fc(out)\n",
    "#         return(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cc5f87a1e87b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T02:31:28.156497Z",
     "start_time": "2023-11-20T02:31:28.152351Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor, self).__init__() # Transpose as well\n",
    "        self.conv1 = nn.Conv2d(1, 2, (1,3), 1, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(2, 4, (1,3), 1, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(4, 8, (1,3))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(8, 12, (1,3))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.FC = nn.Linear(12*16*2, 12)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        flat = torch.flatten(x, 1)\n",
    "        h4 = self.FC(flat)\n",
    "        return h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Predictor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 2, 14, 6]               8\n",
      "              ReLU-2             [-1, 2, 14, 6]               0\n",
      "            Conv2d-3             [-1, 4, 16, 6]              28\n",
      "              ReLU-4             [-1, 4, 16, 6]               0\n",
      "            Conv2d-5             [-1, 8, 16, 4]             104\n",
      "              ReLU-6             [-1, 8, 16, 4]               0\n",
      "            Conv2d-7            [-1, 12, 16, 2]             300\n",
      "              ReLU-8            [-1, 12, 16, 2]               0\n",
      "            Linear-9                   [-1, 12]           4,620\n",
      "================================================================\n",
      "Total params: 5,060\n",
      "Trainable params: 5,060\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(copy.deepcopy(model).to('cpu'), (1, 12, 6))\n",
    "model = model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NNPACK SpatialConvolution_updateOutput failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/19/ltb_35_s78j73zmsywmqs_wc0000gn/T/ipykernel_15393/3867818446.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mchords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/19/ltb_35_s78j73zmsywmqs_wc0000gn/T/ipykernel_15393/1837205373.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NNPACK SpatialConvolution_updateOutput failed"
     ]
    }
   ],
   "source": [
    "best_weights = copy.deepcopy(model.state_dict())\n",
    "max = 0\n",
    "val_acc = 0\n",
    "train_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for i, (chromas,chords) in tqdm(enumerate(train_loader)):\n",
    "        chromas = torch.transpose(chromas,1,2).unsqueeze(1).to(device)\n",
    "        chords = chords.to(device)\n",
    "\n",
    "        preds = model(chromas)\n",
    "        loss = criterion(preds, chords)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step() # Decaying learning rate per 25 epochs by 0.2 times\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}; Loss = {loss.item():.6f}; LR = {scheduler.get_last_lr()}')\n",
    "    with torch.no_grad():\n",
    "        n_samples = 0\n",
    "        n_correct = 0\n",
    "        model.eval()\n",
    "        device = torch.device('cpu')\n",
    "        model = model.to(device)\n",
    "        for chromas, chords in tqdm(test_loader):\n",
    "            chromas = torch.transpose(chromas,1,2).unsqueeze(1).to(device)\n",
    "            chords = chords.to(device)\n",
    "            pred_outputs1 = model(chromas)\n",
    "            prediction = MelChordDataset.predict(model, chromas[0][0], chroma_req=False)\n",
    "            print(prediction)\n",
    "            sys.exit()\n",
    "            # _, actual_preds1 = torch.max(pred_outputs1, 1)\n",
    "            print(actual_preds1.shape)\n",
    "            n_samples += chords.shape[0]\n",
    "            n_correct += (actual_preds1 == chords).sum().item()\n",
    "        val_acc = n_correct/n_samples * 100\n",
    "\n",
    "        if (max <= (n_correct/n_samples * 100)):\n",
    "            print('SAVED MODEL WEIGHTS')\n",
    "            max = val_acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        n_samples = 0\n",
    "        n_correct = 0\n",
    "        \n",
    "        for chromas, chords in train_loader:\n",
    "            chromas = torch.transpose(chromas,1,2).unsqueeze(1).to(device) # From 128, 1, 28, 28 ---> 128, 784\n",
    "            chords = chords.to(device)\n",
    "            pred_outputs1 = model(chromas)\n",
    "            _, actual_preds1 = torch.max(pred_outputs1, 1) # Returns value, index\n",
    "            n_samples += chords.shape[0]\n",
    "            n_correct += (actual_preds1 == chords).sum().item()\n",
    "        train_acc = n_correct/n_samples * 100\n",
    "    \n",
    "    print(f'Train Accuracy: {train_acc:.2f}%')\n",
    "    print(f'Dev Accuracy: {val_acc:.2f}%')\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
