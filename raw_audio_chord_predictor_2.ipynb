{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T20:36:01.263591Z",
     "start_time": "2023-11-19T20:35:59.094962Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf\n",
    "from utils import GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 44100\n",
    "HOP = 256\n",
    "FRAMES = 6\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (gru): GRU(12, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chord_detector = GRU()\n",
    "chord_detector.load_state_dict(torch.load('./models/chord_detector.pth'))\n",
    "chord_detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelChordDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_location = \"../../../../Music Technology/Datasets/musdb18hq/\",\n",
    "            out_location = \"../../../../Music Technology/Datasets/musdb18hq/\",\n",
    "            frames_per_chord = 6,\n",
    "            train = True,\n",
    "            write_data = False\n",
    "        ):\n",
    "        super(MelChordDataset).__init__()\n",
    "        if write_data:\n",
    "            self._write_chords_and_audio(data_location, out_location, train)\n",
    "        self.frames_per_chord = frames_per_chord\n",
    "        vocals_y = []\n",
    "        vocals_chroma = []\n",
    "        chord_templates:dict = json.load(open('./chord_templates.json'))\n",
    "        act_chord_data = []\n",
    "        self.data_location= data_location+\"chunks_vocal/\"\n",
    "        self.out_location= out_location+\"chunks_chord/\"\n",
    "        if not train:\n",
    "            self.data_location[:-1]+=\"_test/\"\n",
    "            self.out_location[:-1]+=\"_test/\"\n",
    "\n",
    "        for i in range(len(os.listdir(self.data_location))): # 100\n",
    "            with open(self.out_location+\"chord_\"+str(i), \"rb\") as fp:\n",
    "                chord_data = pickle.load(fp)\n",
    "            act_chord_data.append(torch.Tensor(np.array([np.array(chord_templates[i]) for i in chord_data])))\n",
    "            vocals_y.append(librosa.load(self.data_location + 'vocal_'+str(i)+'.wav', sr=SR)[0])\n",
    "            vocals_chroma.append(torch.Tensor(librosa.feature.chroma_cens(y=vocals_y[-1], sr = SR, hop_length=HOP)).T)\n",
    "        \n",
    "        # act_chord_data[i]: Shape: (num_chords[i], 12)\n",
    "        # vocals_chroma[i]: Shape: (num_frames[i], 12)\n",
    "        # num_chords[i] = (num_frames[i] // frames_per_chord)\n",
    "\n",
    "        self.data = []\n",
    "        self._create_data(act_chord_data, vocals_chroma)\n",
    "    \n",
    "    def _create_data(self, chord_data, chroma_data):\n",
    "        for (chroma, chords) in zip(chroma_data, chord_data):\n",
    "            for i in range(0, chroma.shape[0]-self.frames_per_chord, self.frames_per_chord):\n",
    "                block_chroma = chroma[i:i+self.frames_per_chord,:]\n",
    "                block_chord = chords[i//self.frames_per_chord]\n",
    "                if(block_chroma.any()):\n",
    "                    self.data.append((block_chroma, block_chord))\n",
    "\n",
    "    def _write_chords_and_audio(\n",
    "            self, \n",
    "            data_location, \n",
    "            out_location, \n",
    "            train = True\n",
    "        ):\n",
    "        if train:\n",
    "            data_location = data_location+\"train/\"\n",
    "        else:\n",
    "            data_location = data_location+\"test/\"\n",
    "        folders = os.listdir(data_location)\n",
    "        count = 0\n",
    "\n",
    "        for folder in folders:\n",
    "            if not os.path.isdir(data_location+folder):\n",
    "                continue\n",
    "            mixture_y, _ = librosa.load(data_location + '/' + folder + '/mixture.wav', sr=SR)\n",
    "            vocals_y, _ = librosa.load(data_location + '/' + folder + '/vocals.wav', sr=SR)\n",
    "            mixture_y = mixture_y/np.max(np.abs(mixture_y))\n",
    "            vocals_y = vocals_y/np.max(np.abs(vocals_y))\n",
    "\n",
    "            mixture_chroma = torch.Tensor(librosa.feature.chroma_cens(y=mixture_y, sr = SR, hop_length=HOP)).T\n",
    "            chunk_length = FRAMES\n",
    "            nchunks = mixture_chroma.shape[0] // chunk_length # no padding\n",
    "\n",
    "            if train:\n",
    "                if not os.path.isdir(out_location+'chunks_chord'):\n",
    "                    os.mkdir(out_location+'chunks_chord')\n",
    "                if not os.path.isdir(out_location+'chunks_vocal'):\n",
    "                    os.mkdir(out_location+'chunks_vocal')\n",
    "            else:\n",
    "                if not os.path.isdir(out_location+'chunks_chord_test/'):\n",
    "                    os.mkdir(out_location+'chunks_chord_test')\n",
    "                if not os.path.isdir(out_location+'chunks_vocal_test/'):\n",
    "                    os.mkdir(out_location+'chunks_vocal_test')\n",
    "\n",
    "            # Get chords from mixture chroma\n",
    "            chord_stack, time = MelChordDataset.prediction(chord_detector, mixture_chroma)\n",
    "            frame_num = np.array([int(i/((HOP/SR)*6)) for i in time])\n",
    "            chord_stack = np.array([frame_num, chord_stack]).T\n",
    "            chords = []\n",
    "            for prev, curr in zip(chord_stack[:-1], chord_stack[1:]):\n",
    "                frame_diff = int(curr[0]) - int(prev[0])\n",
    "                chords.extend([prev[1] for _ in range(frame_diff)])\n",
    "            chords.extend([chord_stack[-1][1] for _ in range(nchunks - len(chords))])\n",
    "\n",
    "            if train:\n",
    "                with open(out_location+\"chunks_chord/chord_\"+str(count), \"wb\") as fp:\n",
    "                    pickle.dump(chords, fp)\n",
    "                sf.write(out_location + 'chunks_vocal/vocal_' + str(count)+'.wav', vocals_y, SR)\n",
    "            else:\n",
    "                with open(out_location+\"chunks_chord_test/chord_\"+str(count), \"wb\") as fp:\n",
    "                    pickle.dump(chords, fp)\n",
    "                sf.write(out_location + 'chunks_vocal_test/vocal_' + str(count)+'.wav', vocals_y, SR)\n",
    "            count+=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(model, audio, chroma_req = True, chord_templates:dict = json.load(open('./chord_templates.json')), sr = SR, hop = HOP):\n",
    "        if chroma_req:\n",
    "            chroma = torch.Tensor(librosa.feature.chroma_cens(y=audio, sr = sr, hop_length=hop)).T.unsqueeze(0)\n",
    "        else:\n",
    "            chroma = audio\n",
    "        with torch.no_grad():\n",
    "            outputs = nn.functional.softmax(model(chroma), 1)[0]\n",
    "        min_val = 120\n",
    "        min_key = ''\n",
    "        for key, val in chord_templates.items():\n",
    "            out = torch.norm(torch.Tensor(val) - outputs)\n",
    "            if min_val >= out:\n",
    "                min_val = out\n",
    "                min_key = key\n",
    "        return min_key\n",
    "    \n",
    "    @staticmethod\n",
    "    def prediction(model, chroma, frame = 6):\n",
    "        stack = []\n",
    "        time = []\n",
    "        model.eval()\n",
    "        pred = MelChordDataset.predict(model, chroma[:frame, :].unsqueeze(0), False)\n",
    "        prev_pred = pred\n",
    "        dur = 1\n",
    "        main_sub = 0\n",
    "        for i in tqdm(range(frame, chroma.shape[0]-frame+1, frame)):\n",
    "            model.eval()\n",
    "            pred = MelChordDataset.predict(model, chroma[i:i+frame, :].unsqueeze(0), False)\n",
    "            if(pred != prev_pred):\n",
    "                if(dur>10):\n",
    "                    if(len(stack)==0):\n",
    "                        stack.append(prev_pred)\n",
    "                    elif(stack[-1]==prev_pred):\n",
    "                        dur = 0\n",
    "                        prev_pred = pred\n",
    "                        continue\n",
    "                    else:\n",
    "                        stack.append(prev_pred)\n",
    "                    if len(time)!=0:\n",
    "                        time.append((i)*HOP/SR - main_sub)\n",
    "                    else:\n",
    "                        main_sub = (i)*HOP/SR\n",
    "                        time.append(0.0)\n",
    "                dur = 0\n",
    "                prev_pred = pred\n",
    "            dur+=1\n",
    "        return stack, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = MelChordDataset(train = True, write_data = False)\n",
    "# test_data = MelChordDataset(train = False, write_data = False)\n",
    "# torch.save(train_data, './data/final/train_data.pt')\n",
    "# torch.save(test_data, './data/final/test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('./data/final/train_data.pt')\n",
    "test_data = torch.load('./data/final/test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576514\n",
      "297423\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6942a376fbccce4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T20:43:41.195866Z",
     "start_time": "2023-11-19T20:43:41.189122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size = 12, hidden_size = 64, num_layers = 1, num_classes = 12, bidirectional = True) -> None:\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True, bidirectional=bidirectional)\n",
    "        if(bidirectional):\n",
    "            self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(self.bidirectional):\n",
    "            h0 = torch.zeros(2*self.num_layers, x.size(0), self.hidden_size)\n",
    "        else:\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:,-1,:] # Since we only want the output of the last cell\n",
    "        out = self.fc(out)\n",
    "        return(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cc5f87a1e87b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T02:31:28.156497Z",
     "start_time": "2023-11-20T02:31:28.152351Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.f_dim = 19\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(12, 24, 5, 2)\n",
    "        self.conv2 = nn.Conv1d(24, 48, 5, 2)\n",
    "        self.conv3 = nn.Conv1d(48, 12, 5, 2)\n",
    "        self.FC = nn.Linear(self.f_dim * 12, 24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.conv1(x))\n",
    "        h2 = self.relu(self.conv2(h1))\n",
    "        h3 = self.relu(self.conv3(h2))\n",
    "        flat = torch.flatten(h3, 1)\n",
    "        h4 = self.FC(flat)\n",
    "        return self.softmax(h4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa691b0b64b1707",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
